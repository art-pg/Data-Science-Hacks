{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/art-pg/Data-Science-Hacks/blob/main/CS388_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbJiCHZHBJ-x"
      },
      "source": [
        "# CS388 Natural Language Processing: Final Project\n",
        "\n",
        "This is the verbose version of the notebook you will using, if you choose to use Colab to train.  \n",
        "\n",
        "We recommend you edit all the information and save your own copy which you can use for all the homeworks.\n",
        "\n",
        "## 1. Set up the GPU\n",
        "\n",
        "First, make sure your colab has access to a GPU.  \n",
        "Select Runtime -> Change runtime type > GPU.\n",
        "\n",
        "Check to see if this works by running the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wazOTw_BxBhl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf6487e4-2f13-4e97-9e84-81a4fa45b20d"
      },
      "source": [
        "import torch\n",
        "\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ6a2n73xU4-"
      },
      "source": [
        "## 2. Sync your code to Colab\n",
        "\n",
        "Set up a **private** GitHub repo w\n",
        "Never use a public repo for your homework, it may lead to plagiarism and you failing the homework and/or class.  \n",
        "\n",
        "Clone this by putting in your information and running the following cell.  \n",
        "\n",
        "For users with 2FA, create a new personal access token at  \n",
        "https://github.com/settings/tokens/new  \n",
        "Select repo and generate your token.  \n",
        "Then enter your personal access token as your password."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOfaBRyuypz3",
        "outputId": "dacd4a30-2db2-44b4-9662-82b6ff7fa6bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ['USER'] = 'art-pg'\n",
        "os.environ['PASS'] = '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'\n",
        "os.environ['REPO'] = 'NLP_Final'\n",
        "\n",
        "!git clone https://$USER:$PASS@github.com/$USER/$REPO.git\n",
        "\n",
        "%cd NLP_Final"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'NLP_Final' already exists and is not an empty directory.\n",
            "/content/NLP_Final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ36HtNTCaDG"
      },
      "source": [
        "## 5. Train your model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Md1unyDkcn9",
        "outputId": "fdef1a2a-04a7-465c-e596-2f5ac6b04b89"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "helpers.py  README.md  requirements.txt  run.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade pip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XluMxZVDSx45",
        "outputId": "fe4dd2f5-36a4-4a5c-9f0e-83bc480367fb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-23.3.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-23.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vr4bH6pZS4kl",
        "outputId": "5b9b7ab2-0808-47d7-9f29-3374ddcef4d6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate (from -r requirements.txt (line 1))\n",
            "  Downloading accelerate-0.24.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting datasets (from -r requirements.txt (line 2))\n",
            "  Downloading datasets-2.14.6-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.66.1)\n",
            "Collecting transformers (from -r requirements.txt (line 5))\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl.metadata (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.1/123.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (6.0.1)\n",
            "Collecting huggingface-hub (from accelerate->-r requirements.txt (line 1))\n",
            "  Downloading huggingface_hub-0.19.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets->-r requirements.txt (line 2))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (3.4.1)\n",
            "Collecting multiprocess (from datasets->-r requirements.txt (line 2))\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets->-r requirements.txt (line 2)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (3.8.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 5)) (2023.6.3)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers->-r requirements.txt (line 5))\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 5))\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 2)) (2023.7.22)\n",
            "Collecting huggingface-hub (from accelerate->-r requirements.txt (line 1))\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 3)) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 2)) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->-r requirements.txt (line 2)) (1.16.0)\n",
            "Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: safetensors, dill, multiprocess, huggingface-hub, tokenizers, accelerate, transformers, datasets\n",
            "Successfully installed accelerate-0.24.1 datasets-2.14.6 dill-0.3.7 huggingface-hub-0.17.3 multiprocess-0.70.15 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJe9KP2cClUN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d7d11c8-d1e2-465b-bed3-f5427a8571f8"
      },
      "source": [
        "!python3 run.py --do_train --task nli --dataset snli --output_dir ./trained_model/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-11 02:53:43.839340: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-11 02:53:43.839397: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-11 02:53:43.839437: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-11 02:53:45.706915: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading builder script: 100% 3.82k/3.82k [00:00<00:00, 14.2MB/s]\n",
            "Downloading metadata: 100% 1.90k/1.90k [00:00<00:00, 12.4MB/s]\n",
            "Downloading readme: 100% 14.1k/14.1k [00:00<00:00, 38.8MB/s]\n",
            "Downloading: 100% 1.93k/1.93k [00:00<00:00, 9.70MB/s]\n",
            "Downloading: 100% 1.26M/1.26M [00:00<00:00, 3.04MB/s]\n",
            "Downloading: 100% 65.9M/65.9M [00:02<00:00, 30.3MB/s]\n",
            "Downloading: 100% 1.26M/1.26M [00:00<00:00, 3.04MB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 665/665 [00:00<00:00, 3.85MB/s]\n",
            "Downloading pytorch_model.bin: 100% 54.2M/54.2M [00:00<00:00, 323MB/s]\n",
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Downloading (…)okenizer_config.json: 100% 29.0/29.0 [00:00<00:00, 173kB/s]\n",
            "Downloading (…)solve/main/vocab.txt: 100% 232k/232k [00:00<00:00, 1.43MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 466k/466k [00:00<00:00, 5.69MB/s]\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "Filter: 100% 10000/10000 [00:00<00:00, 80312.80 examples/s]\n",
            "Filter: 100% 550152/550152 [00:03<00:00, 145540.92 examples/s]\n",
            "Filter: 100% 10000/10000 [00:00<00:00, 141346.09 examples/s]\n",
            "Map (num_proc=2): 100% 549367/549367 [01:51<00:00, 4913.35 examples/s]\n",
            "  0% 0/206013 [00:00<?, ?it/s]You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 0.9461, 'learning_rate': 4.987864843480751e-05, 'epoch': 0.01}\n",
            "{'loss': 0.7303, 'learning_rate': 4.975729686961503e-05, 'epoch': 0.01}\n",
            "{'loss': 0.6862, 'learning_rate': 4.963594530442254e-05, 'epoch': 0.02}\n",
            "{'loss': 0.6477, 'learning_rate': 4.951459373923005e-05, 'epoch': 0.03}\n",
            "{'loss': 0.6304, 'learning_rate': 4.9393242174037564e-05, 'epoch': 0.04}\n",
            "{'loss': 0.5976, 'learning_rate': 4.927189060884507e-05, 'epoch': 0.04}\n",
            "{'loss': 0.6035, 'learning_rate': 4.9150539043652585e-05, 'epoch': 0.05}\n",
            "{'loss': 0.6002, 'learning_rate': 4.90291874784601e-05, 'epoch': 0.06}\n",
            "{'loss': 0.5852, 'learning_rate': 4.890783591326761e-05, 'epoch': 0.07}\n",
            "{'loss': 0.5841, 'learning_rate': 4.8786484348075126e-05, 'epoch': 0.07}\n",
            "{'loss': 0.5713, 'learning_rate': 4.866513278288264e-05, 'epoch': 0.08}\n",
            "{'loss': 0.5643, 'learning_rate': 4.8543781217690146e-05, 'epoch': 0.09}\n",
            "{'loss': 0.5746, 'learning_rate': 4.842242965249766e-05, 'epoch': 0.09}\n",
            "{'loss': 0.5591, 'learning_rate': 4.8301078087305174e-05, 'epoch': 0.1}\n",
            "{'loss': 0.5489, 'learning_rate': 4.817972652211268e-05, 'epoch': 0.11}\n",
            "{'loss': 0.539, 'learning_rate': 4.80583749569202e-05, 'epoch': 0.12}\n",
            "{'loss': 0.5506, 'learning_rate': 4.793702339172771e-05, 'epoch': 0.12}\n",
            "{'loss': 0.5295, 'learning_rate': 4.781567182653522e-05, 'epoch': 0.13}\n",
            "{'loss': 0.5492, 'learning_rate': 4.7694320261342736e-05, 'epoch': 0.14}\n",
            "{'loss': 0.5523, 'learning_rate': 4.757296869615024e-05, 'epoch': 0.15}\n",
            "{'loss': 0.5294, 'learning_rate': 4.7451617130957756e-05, 'epoch': 0.15}\n",
            "{'loss': 0.5461, 'learning_rate': 4.733026556576527e-05, 'epoch': 0.16}\n",
            "{'loss': 0.5438, 'learning_rate': 4.720891400057278e-05, 'epoch': 0.17}\n",
            "{'loss': 0.5318, 'learning_rate': 4.70875624353803e-05, 'epoch': 0.17}\n",
            "{'loss': 0.5219, 'learning_rate': 4.6966210870187804e-05, 'epoch': 0.18}\n",
            "{'loss': 0.524, 'learning_rate': 4.684485930499532e-05, 'epoch': 0.19}\n",
            "{'loss': 0.5236, 'learning_rate': 4.672350773980283e-05, 'epoch': 0.2}\n",
            "{'loss': 0.4963, 'learning_rate': 4.660215617461034e-05, 'epoch': 0.2}\n",
            "{'loss': 0.5237, 'learning_rate': 4.648080460941785e-05, 'epoch': 0.21}\n",
            "{'loss': 0.5185, 'learning_rate': 4.6359453044225366e-05, 'epoch': 0.22}\n",
            "{'loss': 0.5016, 'learning_rate': 4.623810147903288e-05, 'epoch': 0.23}\n",
            "{'loss': 0.5081, 'learning_rate': 4.611674991384039e-05, 'epoch': 0.23}\n",
            "{'loss': 0.5046, 'learning_rate': 4.599539834864791e-05, 'epoch': 0.24}\n",
            "{'loss': 0.5092, 'learning_rate': 4.5874046783455414e-05, 'epoch': 0.25}\n",
            "{'loss': 0.5313, 'learning_rate': 4.575269521826293e-05, 'epoch': 0.25}\n",
            "{'loss': 0.5157, 'learning_rate': 4.563134365307044e-05, 'epoch': 0.26}\n",
            "{'loss': 0.484, 'learning_rate': 4.550999208787795e-05, 'epoch': 0.27}\n",
            "{'loss': 0.5051, 'learning_rate': 4.538864052268547e-05, 'epoch': 0.28}\n",
            "{'loss': 0.4988, 'learning_rate': 4.5267288957492976e-05, 'epoch': 0.28}\n",
            "{'loss': 0.5094, 'learning_rate': 4.514593739230048e-05, 'epoch': 0.29}\n",
            "{'loss': 0.4929, 'learning_rate': 4.5024585827108e-05, 'epoch': 0.3}\n",
            "{'loss': 0.4915, 'learning_rate': 4.490323426191551e-05, 'epoch': 0.31}\n",
            "{'loss': 0.4971, 'learning_rate': 4.4781882696723024e-05, 'epoch': 0.31}\n",
            "{'loss': 0.4975, 'learning_rate': 4.466053113153054e-05, 'epoch': 0.32}\n",
            "{'loss': 0.5046, 'learning_rate': 4.4539179566338044e-05, 'epoch': 0.33}\n",
            "{'loss': 0.4788, 'learning_rate': 4.4417828001145565e-05, 'epoch': 0.33}\n",
            "{'loss': 0.5014, 'learning_rate': 4.429647643595307e-05, 'epoch': 0.34}\n",
            "{'loss': 0.4984, 'learning_rate': 4.4175124870760585e-05, 'epoch': 0.35}\n",
            "{'loss': 0.4681, 'learning_rate': 4.40537733055681e-05, 'epoch': 0.36}\n",
            "{'loss': 0.5131, 'learning_rate': 4.393242174037561e-05, 'epoch': 0.36}\n",
            "{'loss': 0.497, 'learning_rate': 4.381107017518312e-05, 'epoch': 0.37}\n",
            "{'loss': 0.4889, 'learning_rate': 4.368971860999063e-05, 'epoch': 0.38}\n",
            "{'loss': 0.5065, 'learning_rate': 4.356836704479815e-05, 'epoch': 0.39}\n",
            "{'loss': 0.473, 'learning_rate': 4.3447015479605654e-05, 'epoch': 0.39}\n",
            "{'loss': 0.4976, 'learning_rate': 4.3325663914413174e-05, 'epoch': 0.4}\n",
            "{'loss': 0.4798, 'learning_rate': 4.320431234922068e-05, 'epoch': 0.41}\n",
            "{'loss': 0.4724, 'learning_rate': 4.3082960784028195e-05, 'epoch': 0.42}\n",
            "{'loss': 0.4827, 'learning_rate': 4.296160921883571e-05, 'epoch': 0.42}\n",
            "{'loss': 0.5023, 'learning_rate': 4.2840257653643216e-05, 'epoch': 0.43}\n",
            "{'loss': 0.4785, 'learning_rate': 4.2718906088450736e-05, 'epoch': 0.44}\n",
            "{'loss': 0.4782, 'learning_rate': 4.259755452325824e-05, 'epoch': 0.44}\n",
            "{'loss': 0.4617, 'learning_rate': 4.247620295806575e-05, 'epoch': 0.45}\n",
            "{'loss': 0.4669, 'learning_rate': 4.235485139287327e-05, 'epoch': 0.46}\n",
            "{'loss': 0.486, 'learning_rate': 4.223349982768078e-05, 'epoch': 0.47}\n",
            "{'loss': 0.4889, 'learning_rate': 4.211214826248829e-05, 'epoch': 0.47}\n",
            "{'loss': 0.4843, 'learning_rate': 4.1990796697295805e-05, 'epoch': 0.48}\n",
            "{'loss': 0.4653, 'learning_rate': 4.186944513210331e-05, 'epoch': 0.49}\n",
            "{'loss': 0.4818, 'learning_rate': 4.1748093566910825e-05, 'epoch': 0.5}\n",
            "{'loss': 0.4728, 'learning_rate': 4.162674200171834e-05, 'epoch': 0.5}\n",
            "{'loss': 0.4582, 'learning_rate': 4.150539043652585e-05, 'epoch': 0.51}\n",
            "{'loss': 0.479, 'learning_rate': 4.1384038871333367e-05, 'epoch': 0.52}\n",
            "{'loss': 0.4779, 'learning_rate': 4.126268730614088e-05, 'epoch': 0.52}\n",
            "{'loss': 0.4555, 'learning_rate': 4.114133574094839e-05, 'epoch': 0.53}\n",
            "{'loss': 0.4634, 'learning_rate': 4.10199841757559e-05, 'epoch': 0.54}\n",
            "{'loss': 0.4897, 'learning_rate': 4.0898632610563415e-05, 'epoch': 0.55}\n",
            "{'loss': 0.4763, 'learning_rate': 4.077728104537092e-05, 'epoch': 0.55}\n",
            "{'loss': 0.4586, 'learning_rate': 4.065592948017844e-05, 'epoch': 0.56}\n",
            "{'loss': 0.471, 'learning_rate': 4.053457791498595e-05, 'epoch': 0.57}\n",
            "{'loss': 0.4616, 'learning_rate': 4.041322634979346e-05, 'epoch': 0.58}\n",
            "{'loss': 0.4785, 'learning_rate': 4.0291874784600976e-05, 'epoch': 0.58}\n",
            "{'loss': 0.45, 'learning_rate': 4.017052321940848e-05, 'epoch': 0.59}\n",
            "{'loss': 0.4601, 'learning_rate': 4.0049171654216e-05, 'epoch': 0.6}\n",
            "{'loss': 0.4391, 'learning_rate': 3.992782008902351e-05, 'epoch': 0.6}\n",
            "{'loss': 0.4455, 'learning_rate': 3.980646852383102e-05, 'epoch': 0.61}\n",
            "{'loss': 0.4652, 'learning_rate': 3.968511695863854e-05, 'epoch': 0.62}\n",
            "{'loss': 0.439, 'learning_rate': 3.9563765393446045e-05, 'epoch': 0.63}\n",
            "{'loss': 0.4562, 'learning_rate': 3.944241382825356e-05, 'epoch': 0.63}\n",
            "{'loss': 0.4592, 'learning_rate': 3.932106226306107e-05, 'epoch': 0.64}\n",
            "{'loss': 0.4466, 'learning_rate': 3.9199710697868586e-05, 'epoch': 0.65}\n",
            "{'loss': 0.4413, 'learning_rate': 3.907835913267609e-05, 'epoch': 0.66}\n",
            "{'loss': 0.4521, 'learning_rate': 3.8957007567483607e-05, 'epoch': 0.66}\n",
            "{'loss': 0.4584, 'learning_rate': 3.883565600229112e-05, 'epoch': 0.67}\n",
            "{'loss': 0.4661, 'learning_rate': 3.8714304437098634e-05, 'epoch': 0.68}\n",
            "{'loss': 0.4651, 'learning_rate': 3.859295287190615e-05, 'epoch': 0.68}\n",
            "{'loss': 0.4482, 'learning_rate': 3.8471601306713655e-05, 'epoch': 0.69}\n",
            "{'loss': 0.4563, 'learning_rate': 3.835024974152117e-05, 'epoch': 0.7}\n",
            "{'loss': 0.4491, 'learning_rate': 3.822889817632868e-05, 'epoch': 0.71}\n",
            "{'loss': 0.4451, 'learning_rate': 3.810754661113619e-05, 'epoch': 0.71}\n",
            "{'loss': 0.4196, 'learning_rate': 3.798619504594371e-05, 'epoch': 0.72}\n",
            "{'loss': 0.4644, 'learning_rate': 3.7864843480751216e-05, 'epoch': 0.73}\n",
            "{'loss': 0.4611, 'learning_rate': 3.774349191555872e-05, 'epoch': 0.74}\n",
            "{'loss': 0.4427, 'learning_rate': 3.7622140350366244e-05, 'epoch': 0.74}\n",
            "{'loss': 0.4692, 'learning_rate': 3.750078878517375e-05, 'epoch': 0.75}\n",
            "{'loss': 0.4398, 'learning_rate': 3.7379437219981264e-05, 'epoch': 0.76}\n",
            "{'loss': 0.4468, 'learning_rate': 3.725808565478878e-05, 'epoch': 0.76}\n",
            "{'loss': 0.4707, 'learning_rate': 3.7136734089596285e-05, 'epoch': 0.77}\n",
            "{'loss': 0.4716, 'learning_rate': 3.7015382524403805e-05, 'epoch': 0.78}\n",
            "{'loss': 0.447, 'learning_rate': 3.689403095921131e-05, 'epoch': 0.79}\n",
            "{'loss': 0.4586, 'learning_rate': 3.6772679394018826e-05, 'epoch': 0.79}\n",
            "{'loss': 0.4608, 'learning_rate': 3.665132782882634e-05, 'epoch': 0.8}\n",
            "{'loss': 0.4529, 'learning_rate': 3.6529976263633853e-05, 'epoch': 0.81}\n",
            "{'loss': 0.4586, 'learning_rate': 3.640862469844136e-05, 'epoch': 0.82}\n",
            "{'loss': 0.4425, 'learning_rate': 3.6287273133248874e-05, 'epoch': 0.82}\n",
            "{'loss': 0.4227, 'learning_rate': 3.616592156805639e-05, 'epoch': 0.83}\n",
            "{'loss': 0.4411, 'learning_rate': 3.6044570002863895e-05, 'epoch': 0.84}\n",
            "{'loss': 0.4517, 'learning_rate': 3.5923218437671415e-05, 'epoch': 0.84}\n",
            "{'loss': 0.4393, 'learning_rate': 3.580186687247892e-05, 'epoch': 0.85}\n",
            "{'loss': 0.4311, 'learning_rate': 3.5680515307286436e-05, 'epoch': 0.86}\n",
            "{'loss': 0.4296, 'learning_rate': 3.555916374209395e-05, 'epoch': 0.87}\n",
            "{'loss': 0.468, 'learning_rate': 3.5437812176901456e-05, 'epoch': 0.87}\n",
            "{'loss': 0.4519, 'learning_rate': 3.531646061170898e-05, 'epoch': 0.88}\n",
            "{'loss': 0.4395, 'learning_rate': 3.5195109046516484e-05, 'epoch': 0.89}\n",
            "{'loss': 0.4347, 'learning_rate': 3.507375748132399e-05, 'epoch': 0.9}\n",
            "{'loss': 0.4292, 'learning_rate': 3.495240591613151e-05, 'epoch': 0.9}\n",
            "{'loss': 0.4331, 'learning_rate': 3.483105435093902e-05, 'epoch': 0.91}\n",
            "{'loss': 0.4192, 'learning_rate': 3.470970278574653e-05, 'epoch': 0.92}\n",
            "{'loss': 0.4551, 'learning_rate': 3.4588351220554045e-05, 'epoch': 0.92}\n",
            "{'loss': 0.4265, 'learning_rate': 3.446699965536155e-05, 'epoch': 0.93}\n",
            "{'loss': 0.418, 'learning_rate': 3.4345648090169066e-05, 'epoch': 0.94}\n",
            "{'loss': 0.4354, 'learning_rate': 3.422429652497658e-05, 'epoch': 0.95}\n",
            "{'loss': 0.4372, 'learning_rate': 3.4102944959784094e-05, 'epoch': 0.95}\n",
            "{'loss': 0.4259, 'learning_rate': 3.398159339459161e-05, 'epoch': 0.96}\n",
            "{'loss': 0.4527, 'learning_rate': 3.386024182939912e-05, 'epoch': 0.97}\n",
            "{'loss': 0.4341, 'learning_rate': 3.373889026420663e-05, 'epoch': 0.98}\n",
            "{'loss': 0.4487, 'learning_rate': 3.361753869901414e-05, 'epoch': 0.98}\n",
            "{'loss': 0.4527, 'learning_rate': 3.3496187133821655e-05, 'epoch': 0.99}\n",
            "{'loss': 0.4164, 'learning_rate': 3.337483556862916e-05, 'epoch': 1.0}\n",
            "{'loss': 0.4242, 'learning_rate': 3.325348400343668e-05, 'epoch': 1.0}\n",
            "{'loss': 0.4023, 'learning_rate': 3.313213243824419e-05, 'epoch': 1.01}\n",
            "{'loss': 0.4129, 'learning_rate': 3.30107808730517e-05, 'epoch': 1.02}\n",
            "{'loss': 0.3973, 'learning_rate': 3.288942930785922e-05, 'epoch': 1.03}\n",
            "{'loss': 0.4058, 'learning_rate': 3.2768077742666724e-05, 'epoch': 1.03}\n",
            "{'loss': 0.4146, 'learning_rate': 3.264672617747424e-05, 'epoch': 1.04}\n",
            "{'loss': 0.405, 'learning_rate': 3.252537461228175e-05, 'epoch': 1.05}\n",
            "{'loss': 0.4107, 'learning_rate': 3.240402304708926e-05, 'epoch': 1.06}\n",
            "{'loss': 0.4138, 'learning_rate': 3.228267148189678e-05, 'epoch': 1.06}\n",
            "{'loss': 0.4136, 'learning_rate': 3.2161319916704286e-05, 'epoch': 1.07}\n",
            "{'loss': 0.3942, 'learning_rate': 3.20399683515118e-05, 'epoch': 1.08}\n",
            "{'loss': 0.4065, 'learning_rate': 3.191861678631931e-05, 'epoch': 1.08}\n",
            "{'loss': 0.4185, 'learning_rate': 3.179726522112683e-05, 'epoch': 1.09}\n",
            "{'loss': 0.4068, 'learning_rate': 3.1675913655934334e-05, 'epoch': 1.1}\n",
            "{'loss': 0.3888, 'learning_rate': 3.155456209074185e-05, 'epoch': 1.11}\n",
            "{'loss': 0.4049, 'learning_rate': 3.143321052554936e-05, 'epoch': 1.11}\n",
            "{'loss': 0.4094, 'learning_rate': 3.131185896035687e-05, 'epoch': 1.12}\n",
            "{'loss': 0.4016, 'learning_rate': 3.119050739516439e-05, 'epoch': 1.13}\n",
            "{'loss': 0.4034, 'learning_rate': 3.1069155829971895e-05, 'epoch': 1.14}\n",
            "{'loss': 0.4121, 'learning_rate': 3.094780426477941e-05, 'epoch': 1.14}\n",
            "{'loss': 0.4233, 'learning_rate': 3.082645269958692e-05, 'epoch': 1.15}\n",
            "{'loss': 0.4051, 'learning_rate': 3.070510113439443e-05, 'epoch': 1.16}\n",
            "{'loss': 0.4199, 'learning_rate': 3.058374956920195e-05, 'epoch': 1.16}\n",
            "{'loss': 0.4038, 'learning_rate': 3.0462398004009457e-05, 'epoch': 1.17}\n",
            "{'loss': 0.4018, 'learning_rate': 3.0341046438816967e-05, 'epoch': 1.18}\n",
            "{'loss': 0.4073, 'learning_rate': 3.021969487362448e-05, 'epoch': 1.19}\n",
            "{'loss': 0.4173, 'learning_rate': 3.009834330843199e-05, 'epoch': 1.19}\n",
            "{'loss': 0.4195, 'learning_rate': 2.997699174323951e-05, 'epoch': 1.2}\n",
            "{'loss': 0.4025, 'learning_rate': 2.985564017804702e-05, 'epoch': 1.21}\n",
            "{'loss': 0.4084, 'learning_rate': 2.9734288612854526e-05, 'epoch': 1.22}\n",
            "{'loss': 0.3977, 'learning_rate': 2.9612937047662043e-05, 'epoch': 1.22}\n",
            "{'loss': 0.3934, 'learning_rate': 2.9491585482469553e-05, 'epoch': 1.23}\n",
            "{'loss': 0.4048, 'learning_rate': 2.9370233917277067e-05, 'epoch': 1.24}\n",
            "{'loss': 0.4067, 'learning_rate': 2.9248882352084577e-05, 'epoch': 1.25}\n",
            "{'loss': 0.42, 'learning_rate': 2.9127530786892094e-05, 'epoch': 1.25}\n",
            "{'loss': 0.4301, 'learning_rate': 2.9006179221699604e-05, 'epoch': 1.26}\n",
            "{'loss': 0.3981, 'learning_rate': 2.888482765650711e-05, 'epoch': 1.27}\n",
            "{'loss': 0.3901, 'learning_rate': 2.876347609131463e-05, 'epoch': 1.27}\n",
            "{'loss': 0.3876, 'learning_rate': 2.864212452612214e-05, 'epoch': 1.28}\n",
            "{'loss': 0.419, 'learning_rate': 2.8520772960929652e-05, 'epoch': 1.29}\n",
            "{'loss': 0.4097, 'learning_rate': 2.8399421395737163e-05, 'epoch': 1.3}\n",
            "{'loss': 0.3907, 'learning_rate': 2.8278069830544673e-05, 'epoch': 1.3}\n",
            " 44% 89988/206013 [1:23:22<1:24:15, 22.95it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run.py --do_eval --task nli --dataset snli --model ./trained_model/ --output_dir ./eval_output/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0xV7Tk1GRIJ",
        "outputId": "6cfd6b77-392a-470d-81f0-d0c6d98eaa73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'homework.utils' found in sys.modules after import of package 'homework', but prior to execution of 'homework.utils'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "/content/cs342/homework3/homework/dense_transforms.py:86: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "  return torch.as_tensor(np.array(lbl, np.uint8, copy=False))\n",
            "<Figure size 640x480 with 30 Axes>\n",
            "[52.68365479  2.92911214 43.52989014  0.44618978  0.41115316]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV5shP5b2zSa"
      },
      "source": [
        "## 7. Update code and retrain\n",
        "\n",
        "Here there are two options -\n",
        "\n",
        "### Option 1: Code on your local machine\n",
        "\n",
        "* Modify your code locally\n",
        "* Push to Github from your machine\n",
        "* Pull on Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj4vnJWrADPy"
      },
      "source": [
        "!git pull origin master"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrcQpITQAGvA"
      },
      "source": [
        "### Option 2: Code directly on Colab\n",
        "\n",
        "* Modify your code on Colab through the left panel and double-clicking the file\n",
        "* Push to Github from Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deIpj_TTANnW"
      },
      "source": [
        "!git add NLP_Final/*.*\n",
        "!git config --global user.email \"posenato@utexas.edu\"\n",
        "!git config --global user.name \"art-pg\"\n",
        "!git commit -m \"Updating project\"\n",
        "!git push origin master"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /NLP_Final/trained_model.zip /NLP_Final/trained_model"
      ],
      "metadata": {
        "id": "_fO-i0_DhrF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "973WwC5u0ulC"
      },
      "source": [
        "Using your updated code, loop back to step 5 and retrain until you're satisfied."
      ]
    }
  ]
}